{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, fasttext, smart_open\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from wefe.query import Query\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from wefe.metrics.WEAT import WEAT\n",
    "from wefe.metrics.RND import RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "fb_data = pd.read_csv(\"facebook_posts.csv\", engine='python', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "############### PREPROCESSING ################\n",
    "##############################################\n",
    "\n",
    "fb_data [\"Message_proc\"] = fb_data[\"Message\"].astype(str) \n",
    "\n",
    "#remove html markup\n",
    "text = text.str.replace('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});',' ')\n",
    "\n",
    "#remove tabs\n",
    "text= text.str.replace('[ |\\t]{2,}', ' ', case=False)\n",
    "\n",
    "#remove urls \n",
    "text= text.str.replace('http\\S+|www.\\S+', ' ', case=False)\n",
    "\n",
    "#remove e-mails\n",
    "text= text.str.replace('\\\\S+@\\\\S+', ' ', case=False)\n",
    "\n",
    "#remove mentions\n",
    "text= text.str.replace('@\\\\S+', ' ', case=False)\n",
    "\n",
    "#remove hashtags\n",
    "text= text.str.replace('#\\\\S+', ' ', case=False)\n",
    "\n",
    "#remove additional links\n",
    "text= text.str.replace('bit\\\\.ly\\\\S+', ' ', case=False)\n",
    "\n",
    "#save the texts for qualitative part of the analysis\n",
    "fb_data['Message_nice'] = text\n",
    "\n",
    "#remove characters after semicolon (e.g. TV:n)\n",
    "text= text.str.replace('\\\\:[A-Za-z]{1}', ' ', case=False)\n",
    "\n",
    "#split tokens with \"-\"\n",
    "text= text.str.replace('-', ' ')\n",
    "\n",
    "#lowercase\n",
    "text = text.str.lower()\n",
    "\n",
    "#remove numbers and punctuation\n",
    "text = text.str.replace('[^a-zåöä ]',' ').str.replace(' +',' ').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "with open('stopwords_sv.txt', encoding = 'utf-8') as f: stop_words = f.read()\n",
    "\n",
    "stop_words = stop_words.split()\n",
    "\n",
    "customary_stop_words = [\"osv\", \"pga\", \"the\", \"dvs\", \n",
    "                        \"iaf\", \"iom\", \"etc\", \"hej\",\n",
    "                        \"of\", \"obs\", \"iofs\", \"bl\", \"bla\",\n",
    "                        \"sen\", \"hos\", \"via\", \"kl\", \"både\",\n",
    "                        \"mm\", \"per\", \"ex\", \"ca\", \"tex\", \n",
    "                        \"to\", \"and\", \"for\", \"tom\", \"sej\", \n",
    "                        \"dej\", \"mej\", \"mfl\", \"dom\", \"haha\",\n",
    "                        \"truncated\", \"oxå\", \"vet\", \"tycker\",\n",
    "                        \"tror\", \"ex\", \"http\", \"https\", \"www\",\n",
    "                        \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \n",
    "                        \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
    "                        \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\",\n",
    "                        \"w\", \"x\", \"y\", \"z\", \"å\", \"ö\", \"ä\"]\n",
    "\n",
    "stop_words = set(stop_words + customary_stop_words)\n",
    "\n",
    "text=text.apply(lambda x: [item for item in x.split() if item not in stop_words])\n",
    "\n",
    "#remove very short messages\n",
    "fb_data['Message_proc'] = text\n",
    "fb_data[\"WordsCount\"]=fb_data['Message_proc'].apply(lambda x: len(x))\n",
    "fb_data = fb_data[fb_data[\"WordsCount\"]>= 5]\n",
    "\n",
    "#merge lists of tokens into strings\n",
    "fb_data['proc_tokens'] = [' '.join(map(str, l)) for l in fb_data['Message_proc']]\n",
    "\n",
    "#remove duplicated messages one more time\n",
    "fb_data.drop_duplicates(subset=['proc_tokens'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove texts in languages other than Swedish\n",
    "\n",
    "#load the model\n",
    "fasttext_model = fasttext.load_model('lid.176.ftz')\n",
    "\n",
    "docs = fb_data['proc_tokens'].to_list()\n",
    "\n",
    "#run the model\n",
    "docs_lang_fasttext = list(map(fasttext_model.predict, docs)) \n",
    "\n",
    "#extract labels\n",
    "docs_lang_fasttext_labels = [x[0] for x in docs_lang_fasttext]\n",
    "\n",
    "docs_lang_fasttext_labels = [''.join(x) for x in docs_lang_fasttext_labels] \n",
    "docs_lang_fasttext_labels = [x[-2:] for x in docs_lang_fasttext_labels]\n",
    "\n",
    "#remove non-swedish documents\n",
    "fb_data['fasttext'] = docs_lang_fasttext_labels\n",
    "\n",
    "fb_data = fb_data[(fb_data['fasttext'] == \"sv\" )]\n",
    "\n",
    "fb_data = fb_data.reset_index(drop = True)\n",
    "\n",
    "text = fb_data['Message_proc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "############### DOC2VEC MODEL ################\n",
    "##############################################\n",
    "\n",
    "smart_open.open = smart_open.smart_open\n",
    "\n",
    "docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(text)]\n",
    "\n",
    "#train the model with the best parameters based on the QVEC-CCA scores\n",
    "model = Doc2Vec(vector_size=300, min_count=5, epochs=20, dm= 0, window = 8, dbow_words=1)\n",
    "model.build_vocab(docs)\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "#save the model to file\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model.save(fname)\n",
    "\n",
    "#model = Doc2Vec.load('my_doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "###############   RND METRIC  ################\n",
    "##############################################\n",
    "\n",
    "#compile neutral word lists\n",
    "economy = ['skattebetalare', 'arbetslöshet', 'bidrag', 'arbetskraft', 'välfärd', 'jobb', 'arbete', 'skatter','löner',\n",
    "           'socialbidrag', 'kostnad', 'ekonomi', 'anställning', 'företag', 'arbetsförmedlingen', 'tjänst','vinst', \n",
    "           'förlust', 'pengar','medel', 'sysselsättning', 'låglönejobb', 'arbetslösa', 'lönsam', 'kostsam', 'pensioner',\n",
    "           'pensionärer', 'försörja', 'småföretagare', 'bnp', 'försörjning', 'finansiera','arbetsplatser', 'industri']\n",
    "\n",
    "threat = ['kriminalitet','kriminella', 'illegala', 'olagligt', 'terrorister', 'brott', 'våld', 'isis', 'anfall',\n",
    "          'polis', 'våldsam', 'ungdomsgäng', 'förstöra', 'skada', 'angripa', 'lagförbrytare', 'straff', 'farlig',\n",
    "          'våldtäktsmän', 'våldtäckter', 'brottslingar', 'brottslighet', 'kriminalitet', 'småkriminella', 'angripa',\n",
    "          'hot', 'hotande', 'angrepp', 'förstöra', 'skador', 'rädd', 'rädsla', 'risk', 'utvisning']\n",
    "\n",
    "solidarity = ['stödja', 'hjälpa', 'stöd', 'hjälp', 'solidaritet', 'medmänsklighet', 'medmänniskor', 'etablering', \n",
    "              'nöd', 'utsatta', 'skydd', 'mångfald', 'mottagande', 'inkludera', 'integrera', 'mångkultur',\n",
    "              'skyddsbehövande', 'drabbade', 'hjälpbehövande', 'nödlidande', 'inkludering', 'välkommen',\n",
    "              'integration', 'stötta', 'gemensam', 'gemenskap', 'medkänsla', 'sympati', 'deltagande']\n",
    "\n",
    "#compile the \"context\" word lists \n",
    "\n",
    "refugees = ['flykting', 'flyktingar', 'flyktingen', 'flyktingarna', 'asylsökande', 'asylsökanden', 'asylsökandena']\n",
    "\n",
    "immigrants = ['migrant', 'migranter', 'migranten', 'migranterna', 'invandrare', 'invandraren', 'invandarna',\n",
    "              'immigrant', 'immigranter', 'immigranten', 'immigranterna']\n",
    "\n",
    "wefe_model = WordEmbeddingModel(model.wv)\n",
    "rnd = RND()\n",
    "\n",
    "#run the queries\n",
    "query_ec = Query([refugees, immigrants], [economy])\n",
    "result_ec = rnd.run_query(query_ec, wefe_model)\n",
    "\n",
    "query_th = Query([refugees, immigrants], [threat])\n",
    "result_th = rnd.run_query(query_th, wefe_model)\n",
    "\n",
    "query_so = Query([refugees, immigrants], [solidarity])\n",
    "result_so = rnd.run_query(query_so, wefe_model)\n",
    "\n",
    "#print the results\n",
    "print(pd.DataFrame(result_ec, \"\\n\", pd.DataFrame(result_th), \"\\n\", pd.DataFrame(result_so))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "############## SEMANTIC NETWORK ##############\n",
    "##############################################\n",
    "\n",
    "target = ['flyktingar', 'invandrare']\n",
    "\n",
    "#extract the words closest to the target labels using cosine similarity measure\n",
    "edges = []    \n",
    "for i in range (0, len(target)):\n",
    "    x = pd.DataFrame(model.wv.most_similar(target[i], topn = 30), columns = [\"target\", \"similarity\"])\n",
    "    x['source']= target[i]\n",
    "    x = x[x['similarity']>0.55]\n",
    "    x = x.drop (columns = 'similarity')\n",
    "    edges.append(x) \n",
    "\n",
    "edges_df = pd.concat(edges)\n",
    "a=edges_df.target.unique().tolist()\n",
    "b=edges_df.source.unique().tolist()\n",
    "vertices = a+b\n",
    "\n",
    "#extract the words closest to the target labels' closest neigbour words \n",
    "edges_context = []    \n",
    "for i in range (0, len(vertices)):\n",
    "    x = pd.DataFrame(model.wv.most_similar(vertices[i], topn = 30), columns = [\"target\", \"similarity\"])\n",
    "    x['source']= vertices[i]\n",
    "    x = x[x['similarity']>0.55]\n",
    "    x = x.drop (columns = 'similarity')\n",
    "    edges_context.append(x) \n",
    "\n",
    "edges_context_df = pd.concat(edges_context)\n",
    "\n",
    "#compile a complete edge list\n",
    "edges_context_df = pd.concat ([edges_context_df, edges_df])\n",
    "\n",
    "#save the swedish version of the edge list\n",
    "edges_context_df.to_csv('edges_context_df.csv')\n",
    "\n",
    "#upload the translated version of the edge list\n",
    "edges_context_list = pd.read_csv(\"edges_context_list_eng.csv\", engine='python', encoding = 'utf-8')\n",
    "edges_context_list = edges_context_list.drop(columns = 'N')\n",
    "\n",
    "#create vertex labels\n",
    "vertices_labels =  pd.unique(edges_context_list[[\" source\", \" target\"]].values.ravel()).tolist()\n",
    "vertices_labels = {a : a for a in vertices_labels}\n",
    "\n",
    "edges_context_list = edges_context_list.to_records(index=False)\n",
    "\n",
    "#create a network object\n",
    "net = nx.Graph()\n",
    "net.add_edges_from(edges_context_list)\n",
    "\n",
    "#create a colour map for the target labels and context words\n",
    "color_map = []\n",
    "for node in net3:\n",
    "    if node == ' immigrants(2)' or node == ' refugees':\n",
    "        color_map.append('#fc9272')\n",
    "    else: \n",
    "        color_map.append('lightblue')  \n",
    "\n",
    "pos = nx.kamada_kawai_layout(net, scale = 5)\n",
    "plt.figure(10, figsize=(20,15)) \n",
    "\n",
    "nx.draw_networkx_nodes(net, pos, node_color=color_map, node_size = 800)\n",
    "nx.draw_networkx_edges(net, pos, width=1.0, alpha=0.5)\n",
    "nx.draw_networkx_labels(net, pos, labels=vertices_labels) \n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.draw() \n",
    "#plt.savefig('sem_net.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "########### QUALITATIVE ANALYSIS #############\n",
    "##############################################\n",
    "\n",
    "#identify documents located closest to the target labels\n",
    "\n",
    "similar_docs_i = model.docvecs.most_similar(positive=[model.wv['invandrare']], topn = 50)\n",
    "similar_docs_f = model.docvecs.most_similar(positive=[model.wv['flytkingar']], topn = 50)\n",
    "\n",
    "inds_i = [i[0] for i in similar_docs_i]\n",
    "inds_f = [i[0] for i in similar_docs_f]\n",
    "\n",
    "display(HTML(fb_data['Message_nice'][fb_data.index.isin(inds_i)].to_frame().to_html()))\n",
    "display(HTML(fb_data['Message_nice'][fb_data.index.isin(inds_f)].to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "############# DOCUMENT CLUSTERS ##############\n",
    "##############################################\n",
    "\n",
    "#determine the optimal number of clusters\n",
    "\n",
    "kmeans_kwargs = {\n",
    "       \"init\": \"random\",\n",
    "       \"n_init\": 10,\n",
    "       \"max_iter\": 300,\n",
    "       \"random_state\": 180404,\n",
    "   }\n",
    "\n",
    "#use the elbow method\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(model.docvecs.vectors_docs)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "#plot the results\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"The optimal number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the optimal number of clusters analytically\n",
    "\n",
    "#choose the number of clusters to be evaluated\n",
    "n_clus = [6, 8, 10, 12]\n",
    "clusters=[]\n",
    "\n",
    "for x in range (0, len(n_clus)):\n",
    "    #run the model with x clusters\n",
    "    kmeans_model = KMeans(n_clusters= n_clus[x], init='k-means++', random_state = 180404, max_iter=300).fit (model.docvecs.vectors_docs)\n",
    "    cl = kmeans_model.predict(model.docvecs.vectors_docs)\n",
    "    clusters.append(cl)\n",
    "    fb_data['cluster'] = cl\n",
    "    fb_data['cluster'] = fb_data['cluster'].astype(str)\n",
    "    \n",
    "    #identify top-30 words in each of the clusters and solutions based on the tf-idf values\n",
    "    NUMBER_OF_CLUSTERS = n_clus[x]\n",
    "    tf_idf_clusters = []\n",
    "    top_n = 30\n",
    "    for i in range(NUMBER_OF_CLUSTERS):\n",
    "        y = tfidfvectorizer.fit_transform(fb_data[\"Message_proc\"][fb_data['cluster'] == i])\n",
    "        tf_idf_clusters.append(y)\n",
    "        feature_names = tfidfvectorizer.get_feature_names()\n",
    "        print('tf_idf scores: \\n', sorted(list(zip(tfidfvectorizer.get_feature_names(), y.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n])  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-run the model with the optimal number of clusters \n",
    "#and identify the the closest documents for the qualitative part of the analysis\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 8\n",
    "\n",
    "kmeans_model = KMeans(n_clusters= 8, init='k-means++', random_state = 180404, max_iter=300).fit (model.docvecs.vectors_docs)\n",
    "cl = kmeans_model.predict(model.docvecs.vectors_docs)\n",
    "fb_data['cluster'] = cl\n",
    "\n",
    "top_n = 30\n",
    "for i in range(NUMBER_OF_CLUSTERS):\n",
    "    y = tfidfvectorizer.fit_transform(fb_data[\"proc_tokens\"][fb_data['cluster'] == i])\n",
    "    feature_names = tfidfvectorizer.get_feature_names()\n",
    "    print('tf_idf scores: \\n', sorted(list(zip(tfidfvectorizer.get_feature_names(), y.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n])  \n",
    "\n",
    "#fins cluster centroids    \n",
    "centroids = kmeans_model.cluster_centers_\n",
    "\n",
    "#find the documents closest to each of the cluster centroids\n",
    "for i in range(NUMBER_OF_CLUSTERS):\n",
    "    z = model.docvecs.most_similar(positive = [centroids[i]], topn = 50)\n",
    "    df = pd.DataFrame(z, columns = [\"id\", \"score\"])\n",
    "    df['id']=df['id'].astype(str)\n",
    "    display(HTML(fb_data['Message_nice'][fb_data.index.isin(df['id'])].to_frame().to_html()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
